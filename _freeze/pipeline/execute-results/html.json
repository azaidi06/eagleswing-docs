{
  "hash": "24b4faad37de2b5d2a0d0720f53dfa98",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Pipeline Flow\"\nsubtitle: \"How data moves from phone to analysis\"\n---\n\n## S3 Layout\n\nAll data lives in `golf-swing-data` with a golfer-first key structure:\n\n```\n{golfer}/\nâ”œâ”€â”€ raw/          # Original .MOV uploads (iPhone HEVC 10-bit VFR)\nâ”œâ”€â”€ processed/    # Transcoded .mp4 (H.264 CFR yuv420p)\nâ”œâ”€â”€ keypoints/    # ViTPose .pkl files (17 keypoints per frame)\nâ”œâ”€â”€ detection/    # Swing detection .json (backswing + contact frames)\nâ”œâ”€â”€ fingers/      # Finger prediction .json\nâ”œâ”€â”€ frames/       # Extracted JPGs with skeleton overlay\nâ”œâ”€â”€ output/       # Grids, signal plots, hand crops\nâ””â”€â”€ analysis/     # Biomechanical comparison plots + AI text\n```\n\n## Inter-Stage Messaging\n\nStages are connected via **S3 event notifications â†’ SQS**, with no orchestrator:\n\n```{mermaid}\nsequenceDiagram\n    participant Phone\n    participant S3\n    participant SQS\n    participant EC2\n    participant Lambda1 as swing_detection\n    participant Lambda2 as post_processing\n\n    Phone->>S3: Upload .MOV to /raw/\n    S3->>SQS: S3 event (suffix .MOV/.mp4)\n    SQS->>EC2: worker.py polls\n    EC2->>S3: Upload .mp4 to /processed/\n    EC2->>S3: Upload .pkl to /keypoints/\n    S3->>Lambda1: S3 event (suffix .pkl)\n    Lambda1->>S3: Upload .json to /detection/\n    S3->>Lambda2: S3 event (suffix .json)\n    Lambda2->>S3: Upload to /fingers/, /frames/, /output/\n```\n\nEach stage fires-and-forgets to S3, and the next stage picks it up via event triggers. No central coordinator.\n\n## Path Guards & Idempotency\n\nThe label worker has two safety mechanisms:\n\n**Path guards** â€” only processes keys matching `/raw/` or `/processed/`:\n\n::: {#6884541c .cell execution_count=1}\n``` {.python .cell-code}\n# From worker.py â€” process_message()\nif \"/raw/\" not in key and \"/processed/\" not in key:\n    logger.info(\"Skipping non-raw/processed key: %s\", key)\n    return\n```\n:::\n\n\n**Idempotency** â€” skips if .pkl already exists:\n\n::: {#2c2528e5 .cell execution_count=2}\n``` {.python .cell-code}\n# Checks S3 before processing\npkl_key = key.replace(\"/raw/\", \"/keypoints/\").replace(\".MOV\", \".pkl\")\nif s3_object_exists(bucket, pkl_key):\n    logger.info(\"Already labeled: %s\", pkl_key)\n    return\n```\n:::\n\n\n## Timing Breakdown\n\nFor a 5-min 60fps iPhone video (~18,000 frames, 168 MB raw):\n\n::: {#fbd1b646 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\")\n\nstages = [\"S3 download\", \"Transcode\\n(NVENC)\", \"Labeling\\n(turbo)\", \"S3 upload\", \"Swing\\ndetect\", \"Post\\nprocess\"]\ntimes = [15, 74, 313, 35, 13, 5]\ncolors = [\"#95a5a6\", \"#f39c12\", \"#e74c3c\", \"#95a5a6\", \"#3498db\", \"#3498db\"]\n\nfig, ax = plt.subplots(figsize=(10, 3))\nbars = ax.barh(stages, times, color=colors, edgecolor=\"white\", height=0.6)\nax.set_xlabel(\"Seconds\")\nax.set_title(\"Per-video processing time (5-min 60fps, g6.2xlarge)\")\nax.invert_yaxis()\n\nfor bar, t in zip(bars, times):\n    ax.text(bar.get_width() + 3, bar.get_y() + bar.get_height()/2, f\"{t}s\", va=\"center\", fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n| Color | Meaning |\n|-------|---------|\n| ðŸŸ  Orange | GPU hardware (NVENC/NVDEC) |\n| ðŸ”´ Red | GPU compute (CUDA, the bottleneck) |\n| âšª Gray | Network I/O |\n| ðŸ”µ Blue | Lambda CPU |\n\n## Cold Start\n\nFirst video on a fresh instance incurs a one-time cold start:\n\n| Phase | Time | Why |\n|-------|------|-----|\n| Boot + user-data | 20s | cloud-init, S3 script pull |\n| Python imports | 91s | EBS lazy-loading from snapshot (~12 MB/s) |\n| Model load | 15s | Local checkpoint files |\n| **Total** | **~2.1 min** | Amortized over all videos in session |\n\nAfter cold start, inductor compiled models are cached on disk. `deploy.sh` uses `cmp -s` before overwriting worker scripts to preserve file mtimes â†’ inductor cache stays valid across deploys.\n\n",
    "supporting": [
      "pipeline_files"
    ],
    "filters": [],
    "includes": {}
  }
}