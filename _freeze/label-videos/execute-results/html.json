{
  "hash": "b7b8b03fe56eb1d6378f22a975cdf4f4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Label Videos\"\nsubtitle: \"NVENC transcode + ViTPose-Huge pose estimation on EC2 GPU\"\n---\n\n## Overview\n\nThe heaviest stage. Downloads raw iPhone video, transcodes to H.264 CFR, runs batched pose estimation, uploads results. All on a single EC2 spot instance.\n\n```\nworker.py (SQS mode)     worker.py --local <dir>     worker.py --local <dir> -w 3\n    │                         │                            │\n    ▼                         ▼                            ▼\n  poll SQS              process directory              multiprocessing\n  → download .MOV       → iterate .MOV/.mp4            → N GPU workers\n  → transcode           → transcode if needed          → process in parallel\n  → label               → label\n  → upload              → save locally\n```\n\n## Transcode\n\n`transcode_if_needed()` probes the input and decides what to do:\n\n::: {#51d89923 .cell execution_count=1}\n``` {.python .cell-code}\n# Probe pipeline (ffprobe)\n# 1. Check codec — if already H.264, may skip transcode\n# 2. Check VFR — variable frame rate needs CFR conversion\n# 3. Check pixel format — 10-bit HEVC needs yuv420p conversion\n\n# NVENC transcode command (simplified):\n# ffmpeg -hwaccel cuda -i input.MOV \\\n#   -c:v h264_nvenc -preset p4 -pix_fmt yuv420p \\\n#   -r 60 -an output.mp4\n\n# Fallback: libx264 CPU if NVENC unavailable\n```\n:::\n\n\nKey decisions:\n\n- **VFR → CFR**: iPhone shoots variable frame rate. Must normalize for frame-accurate keypoint indexing.\n- **HEVC 10-bit → H.264 8-bit**: Adds `-pix_fmt yuv420p` for downstream compatibility.\n- **NVENC with libx264 fallback**: GPU transcode is ~5x faster, but falls back gracefully.\n\n## Pose Estimation\n\nUses **ViTPose-Huge** (632M params) with **RTMDet-M** as the person detector. Outputs COCO-17 keypoints per frame.\n\n### COCO-17 Keypoint Layout\n\n::: {#f08e3f64 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\")\n\n# COCO-17 keypoints\nnames = [\n    \"nose\", \"L eye\", \"R eye\", \"L ear\", \"R ear\",\n    \"L shoulder\", \"R shoulder\", \"L elbow\", \"R elbow\",\n    \"L wrist\", \"R wrist\", \"L hip\", \"R hip\",\n    \"L knee\", \"R knee\", \"L ankle\", \"R ankle\"\n]\n\n# Approximate positions for visualization\npositions = [\n    (0.50, 0.95),  # nose\n    (0.47, 0.97), (0.53, 0.97),  # eyes\n    (0.43, 0.95), (0.57, 0.95),  # ears\n    (0.38, 0.78), (0.62, 0.78),  # shoulders\n    (0.30, 0.62), (0.70, 0.62),  # elbows\n    (0.25, 0.48), (0.75, 0.48),  # wrists\n    (0.42, 0.50), (0.58, 0.50),  # hips\n    (0.40, 0.30), (0.60, 0.30),  # knees\n    (0.38, 0.10), (0.62, 0.10),  # ankles\n]\n\n# Skeleton connections\nskeleton = [\n    (0,1),(0,2),(1,3),(2,4),  # head\n    (5,6),(5,7),(7,9),(6,8),(8,10),  # arms\n    (5,11),(6,12),(11,12),  # torso\n    (11,13),(13,15),(12,14),(14,16),  # legs\n]\n\nfig, ax = plt.subplots(figsize=(4, 8))\nfor i, j in skeleton:\n    ax.plot([positions[i][0], positions[j][0]],\n            [positions[i][1], positions[j][1]], \"gray\", lw=2, alpha=0.5)\nfor idx, (x, y) in enumerate(positions):\n    color = \"#e74c3c\" if idx in (9, 10) else \"#3498db\"  # highlight wrists\n    ax.scatter(x, y, s=80, c=color, zorder=5)\n    ax.annotate(f\"{idx}: {names[idx]}\", (x, y), fontsize=7,\n                xytext=(8, 0), textcoords=\"offset points\", va=\"center\")\n\nax.set_xlim(0.1, 0.9)\nax.set_ylim(0.0, 1.05)\nax.set_aspect(\"equal\")\nax.set_title(\"COCO-17 Keypoints\\n(red = wrists, used for swing detection)\")\nax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nIndices **9** (left wrist) and **10** (right wrist) are the primary signal for swing detection downstream.\n\n## Inference Modes\n\n### `label_video_hifi` — Serial RTMDet + Batched ViTPose\n\n```\nfor each batch of 32 frames:\n    run RTMDet on each frame (serial)\n    batch all person crops\n    run ViTPose on batch\n```\n\n### `label_video_turbo` — Fully Batched with Threaded Overlap\n\n```{mermaid}\nflowchart LR\n    subgraph BG[\"Background Thread (CPU)\"]\n        A[\"NVDEC decode<br/>h264_cuvid\"] --> B[\"Resize/pad<br/>640x640\"]\n        B --> C[\"Normalize<br/>for RTMDet\"]\n        C --> Q[\"queue.Queue<br/>(maxsize=2)\"]\n    end\n\n    subgraph Main[\"Main Thread (GPU)\"]\n        Q --> D[\"RTMDet batched<br/>(inductor compiled)\"]\n        D --> E[\"Crop + warp<br/>192x256 patches\"]\n        E --> F[\"ViTPose batched<br/>(inductor compiled)\"]\n    end\n```\n\nThe background thread hides **~70ms** of CPU work (decode + resize/pad) behind GPU execution. This is the production mode.\n\n## torch.compile Performance\n\n::: {#02f47763 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport numpy as np\n\nmodels = [\"RTMDet\", \"ViTPose\", \"E2E turbo\\n(fps)\"]\neager = [236, 258, 32.5]\ncompiled = [107, 237, 57.5]\nspeedup = [2.2, 1.08, 1.77]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Bar chart\nbars1 = ax1.bar(x - width/2, eager, width, label=\"Eager\", color=\"#e74c3c\", alpha=0.8)\nbars2 = ax1.bar(x + width/2, compiled, width, label=\"Compiled (inductor)\", color=\"#2ecc71\", alpha=0.8)\nax1.set_ylabel(\"ms/batch (lower=better) | fps (higher=better)\")\nax1.set_xticks(x)\nax1.set_xticklabels(models)\nax1.legend()\nax1.set_title(\"Eager vs torch.compile\")\n\n# Speedup\nax2.bar(models, speedup, color=[\"#f39c12\", \"#f39c12\", \"#e74c3c\"], alpha=0.8)\nax2.axhline(y=1.0, color=\"gray\", linestyle=\"--\", alpha=0.5)\nax2.set_ylabel(\"Speedup (x)\")\nax2.set_title(\"Compilation Speedup\")\nfor i, v in enumerate(speedup):\n    ax2.text(i, v + 0.03, f\"{v}x\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n- **Backend**: `inductor` (default). `torch_tensorrt` was rejected — L4 is bandwidth-bound, TRT FP16 was *slower* than eager.\n- **First-batch overhead**: ~18s for inductor graph lowering + autotuning\n- **Cache**: `TORCHINDUCTOR_FX_GRAPH_CACHE=1`, keyed on code hash + file mtime\n\n## Max Batch Sizes (24GB L4 VRAM)\n\n| Model | Input Size | Max Batch | Notes |\n|-------|-----------|-----------|-------|\n| ViTPose-Huge | 192x256 | 1024 | Small inputs, large batches |\n| RTMDet-M | 640x640 | 448 | VRAM limiter due to input size |\n\nProduction uses batch=32, well within limits.\n\n## Loading a Sample PKL\n\n::: {#8afa9a63 .cell execution_count=4}\n``` {.python .cell-code}\nimport pickle, pathlib, numpy as np\n\n# Find a sample pkl in test_trials\nroot = pathlib.Path(\"..\").resolve()\npkls = sorted(root.glob(\"test_trials/**/keypoints/*.pkl\"))\n\nif pkls:\n    pkl_path = pkls[0]\n    print(f\"Loading: {pkl_path.relative_to(root)}\")\n\n    with open(pkl_path, \"rb\") as f:\n        data = pickle.load(f)\n\n    meta = data.get(\"__meta__\", {})\n    frame_keys = [k for k in data if k != \"__meta__\"]\n    print(f\"Frames: {len(frame_keys)}\")\n    print(f\"Metadata: {meta}\")\n\n    if frame_keys:\n        sample = data[frame_keys[0]]\n        print(f\"\\nPer-frame keys: {list(sample.keys())}\")\n        print(f\"Keypoints shape: {np.array(sample['keypoints']).shape}\")\n        print(f\"Scores shape: {np.array(sample['keypoint_scores']).shape}\")\nelse:\n    print(\"No sample pkl files found in test_trials/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading: test_trials/test0_5min_local/keypoints/IMG_1016.pkl\nFrames: 19750\nMetadata: {'fps': 59.94005994005994, 'total_frames': 19750, 'width': 1080, 'height': 1920, 'n_pkl_frames': 19750}\n\nPer-frame keys: ['keypoints', 'keypoint_scores', 'bbox', 'bbox_score']\nKeypoints shape: (17, 2)\nScores shape: (17,)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "label-videos_files"
    ],
    "filters": [],
    "includes": {}
  }
}